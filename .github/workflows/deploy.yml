name: deploy

on:
  # Run from the Actions tab with optional overrides
  workflow_dispatch:
    inputs:
      s3_key:
        description: 'S3 object key (where to upload)'
        required: false
        default: 'combined_policy_summary.csv'
      dataset_id:
        description: 'QuickSight dataset id (leave blank to use secret)'
        required: false
        default: ''

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps (if any)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # Create a single file we can upload. 
      # Prefers data/policy_usage_report_*.csv (merges them), 
      # falls back to policy_summary.csv at repo root.
      - name: Build combined CSV
        shell: bash
        run: |
          set -euo pipefail
          OUT="combined_policy_summary.csv"

          if ls data/policy_usage_report_*.csv >/dev/null 2>&1; then
            echo "Found per-policy CSVs in data/ — merging..."
            FIRST=$(ls data/policy_usage_report_*.csv | head -n1)
            {
              head -n1 "$FIRST"
              tail -n +2 -q data/policy_usage_report_*.csv
            } > "$OUT"
          elif [ -f policy_summary.csv ]; then
            echo "Found policy_summary.csv at repo root — using it."
            cp policy_summary.csv "$OUT"
          else
            echo "❌ No input CSVs found (data/policy_usage_report_*.csv or policy_summary.csv)."
            echo "    Run your pipeline locally or in CI first, or commit the file."
            exit 1
          fi
          echo "Produced:"
          ls -lh "$OUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ secrets.AWS_REGION }}

      - name: Upload combined CSV to S3
        shell: bash
        run: |
          KEY="${{ github.event.inputs.s3_key }}"
          [ -z "$KEY" ] && KEY="combined_policy_summary.csv"
          echo "Uploading to s3://${{ secrets.S3_BUCKET }}/$KEY"
          aws s3 cp combined_policy_summary.csv "s3://${{ secrets.S3_BUCKET }}/$KEY" --acl private

      - name: Refresh QuickSight dataset
        shell: bash
        run: |
          DS_ID="${{ github.event.inputs.dataset_id }}"
          [ -z "$DS_ID" ] && DS_ID="${{ secrets.QS_DATASET_ID }}"

          echo "Triggering QuickSight ingestion for dataset: $DS_ID"
          aws quicksight create-ingestion \
            --aws-account-id  ${{ secrets.AWS_ACCOUNT_ID }} \
            --data-set-id     "$DS_ID" \
            --ingestion-id    "ing-$(date +%s)" \
            --region          ${{ secrets.AWS_REGION }}
