name: deploy

on:
  workflow_dispatch:        # allow manual runs
  push:                     # automatic on relevant changes to main
    branches: [ main ]
    paths:
      - "data/**/*.csv"
      - "script/**"
      - ".github/workflows/deploy.yml"
  schedule:                 # nightly at 02:00 UTC
    - cron: "0 2 * * *"

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write        # (future-proof if you move to OIDC)
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies (best effort)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt || true
          fi

      # Optional â€“ if your repo has a pipeline that produces CSVs
      - name: (Optional) run your pipeline
        run: |
          if [ -f script/run_all.py ]; then
            python script/run_all.py || true
          fi

      - name: Compose combined CSV
        run: |
          set -euo pipefail
          OUT="combined_policy_summary.csv"

          if ls data/policy_usage_report_*.csv >/dev/null 2>&1; then
            echo "Merging data/policy_usage_report_*.csv -> $OUT"
            head -n 1 $(ls data/policy_usage_report_*.csv | head -n1) > "$OUT"
            for f in data/policy_usage_report_*.csv; do
              tail -n +2 "$f" >> "$OUT"
            done

          elif [ -f data/policy_usage_report.csv ]; then
            echo "Using data/policy_usage_report.csv -> $OUT"
            cp data/policy_usage_report.csv "$OUT"

          elif ls policy_usage_report_*.csv >/dev/null 2>&1; then
            echo "Merging policy_usage_report_*.csv in repo root -> $OUT"
            head -n 1 $(ls policy_usage_report_*.csv | head -n1) > "$OUT"
            for f in policy_usage_report_*.csv; do
              tail -n +2 "$f" >> "$OUT"
            done

          elif [ -f policy_summary.csv ]; then
            echo "Using policy_summary.csv -> $OUT"
            cp policy_summary.csv "$OUT"

          else
            echo "::error::No input CSVs found to compose $OUT"
            echo "Looked for: data/policy_usage_report_*.csv, data/policy_usage_report.csv, policy_usage_report_*.csv, policy_summary.csv"
            exit 1
          fi

          echo "Composed file:"
          ls -l "$OUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ secrets.AWS_REGION }}

      - name: Upload combined CSV to S3
        run: |
          aws s3 cp combined_policy_summary.csv \
            "s3://${{ secrets.S3_BUCKET }}/combined_policy_summary.csv" \
            --acl private
          echo "Uploaded to s3://${{ secrets.S3_BUCKET }}/combined_policy_summary.csv"

      - name: Refresh QuickSight dataset
        run: |
          aws quicksight create-ingestion \
            --aws-account-id  "${{ secrets.AWS_ACCOUNT_ID }}" \
            --data-set-id     "${{ secrets.QS_DATASET_ID }}" \
            --ingestion-id    "ing-$(date +%s)" \
            --region          "${{ secrets.AWS_REGION }}"
          echo "Triggered QuickSight SPICE ingestion for dataset ${{ secrets.QS_DATASET_ID }}"